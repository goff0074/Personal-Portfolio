{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a559624b-011d-46dc-b422-e9deb69f6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 sweeps.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([49.999995093705294, 49.99999551296493], [1.0 0.0 0.0; 1.0 0.0 0.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an implementation of value iteration to solve for the optimal policy\n",
    "# for the recycling robot that is described on page 52 of the RL book.\n",
    "\n",
    "S = [1,2]    # state is lo, hi\n",
    "A = [1,2,3]  # action is search, wait, recharge\n",
    "\n",
    "# transition probabilities when searching\n",
    "# when action = search\n",
    "#       lo   hi\n",
    "#    -------------\n",
    "# lo |  β   1-β\n",
    "# hi |1-α    α\n",
    "α = 0.2\n",
    "β = 0.3\n",
    "r_search = 5.0   # expected reward when searching\n",
    "r_wait = 1.0\n",
    "\n",
    "# the 4-argument transition probabilities p(sprime,r|s,a).\n",
    "# let's try to get by with the 3-argument probabilities p(sprime|s,a)\n",
    "# that are shown in the table on page 52.\n",
    "# the indexing is p[s, a, sprime]\n",
    "p3 = zeros(2, 3, 2)\n",
    "p3[2,1,2] = α\n",
    "p3[2,1,1] = 1-α\n",
    "p3[1,1,2] = 1-β\n",
    "p3[1,1,1] = β\n",
    "p3[2,2,2] = 1\n",
    "p3[1,2,1] = 1\n",
    "p3[1,3,2] = 1\n",
    "# now note that when the state=hi, action=recharge will not be considered.\n",
    "# we will take care of that in the value iteration function.\n",
    "\n",
    "# an implementation of the value iteration algorithm shown on page\n",
    "# 83 in the RL book.\n",
    "function value_iteration(V, π)\n",
    "    γ = 0.9    # discount factor\n",
    "    θ = 1e-6   # tolerance for convergence\n",
    "    k = 0      # iteration/sweep\n",
    "    while true\n",
    "        δ = 0.0\n",
    "        for s in 1:length(S)\n",
    "            v = V[s]  # old value\n",
    "            q = zeros(length(A))  # State-action values\n",
    "            for a in 1:length(A)\n",
    "                if s == 2 && a == 3 #skip actions such as recharging in a high state\n",
    "                    continue\n",
    "                end\n",
    "                reward = (a == 1 ? r_search : r_wait) #reward for actions\n",
    "                for s_prime in 1:length(S)\n",
    "                    q[a] += p3[s, a, s_prime] * (reward + γ * V[s_prime]) #computation of q given rewards and s'\n",
    "                end\n",
    "            end\n",
    "            V[s], best_action = findmax(q) #update value function\n",
    "            π[s, setdiff(A, best_action)] .= 0.0\n",
    "            π[s, best_action] = 1.0 #update policy\n",
    "            δ = max(δ, abs(v - V[s])) #max change\n",
    "        end\n",
    "        k += 1  # Increment iteration counter\n",
    "        if δ < θ\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    println(k, \" sweeps.\")\n",
    "    return V, π\n",
    "end\n",
    "\n",
    "\n",
    "# the policy: rows correspond to states, columns correspond to actions.\n",
    "π = fill(1.0 / length(A), length(S), length(A))\n",
    "V = zeros(length(S))  # will hold the value function\n",
    "V, π = value_iteration(V, π)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
